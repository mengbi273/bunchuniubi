```

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class GroupedQueryAttention(nn.Module):
    """
    A minimal Grouped-Query Attention (GQA) implementation.

    Args:
        d_model (int): model hidden size.
        num_query_heads (int): number of attention heads for queries (Hq).
        num_kv_heads (int): number of attention heads for keys/values (Hkv). Must divide Hq.
        dropout (float): attention dropout probability (default: 0.0).

    Shapes:
        query, key, value: (B, S, d_model)
        output: (B, S, d_model)

    Notes:
        - We use head_dim = d_model // num_query_heads.
        - K and V are projected to Hkv heads, each of size head_dim.
        - Each KV head is *shared* by g = Hq // Hkv query heads.
          We implement this by repeating K/V heads via repeat_interleave(g, dim=2).
    """
    def __init__(self, d_model: int, num_query_heads: int, num_kv_heads: int, dropout: float = 0.0):
        super().__init__()
        assert d_model % num_query_heads == 0, "d_model must be divisible by num_query_heads"
        assert num_query_heads % num_kv_heads == 0, "num_query_heads must be divisible by num_kv_heads"

        self.d_model = d_model
        self.num_query_heads = num_query_heads
        self.num_kv_heads = num_kv_heads
        self.group_size = num_query_heads // num_kv_heads
        self.head_dim = d_model // num_query_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)

        # Projections
        self.W_q = nn.Linear(d_model, num_query_heads * self.head_dim, bias=False)
        self.W_k = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)
        self.W_v = nn.Linear(d_model, num_kv_heads * self.head_dim, bias=False)
        self.W_o = nn.Linear(num_query_heads * self.head_dim, d_model, bias=False)

        self.attn_drop = nn.Dropout(dropout) if dropout and dropout > 0.0 else nn.Identity()

    def _reshape_to_heads(self, x: torch.Tensor, n_heads: int):
        # x: (B, S, n_heads * head_dim) -> (B, n_heads, S, head_dim)
        B, S, _ = x.shape
        x = x.view(B, S, n_heads, self.head_dim)
        x = x.permute(0, 2, 1, 3).contiguous()
        return x

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: torch.Tensor = None):
        # Project
        q = self._reshape_to_heads(self.W_q(query), self.num_query_heads)  # (B, Hq, S, Dh)
        k = self._reshape_to_heads(self.W_k(key),   self.num_kv_heads)     # (B, Hkv, S, Dh)
        v = self._reshape_to_heads(self.W_v(value), self.num_kv_heads)     # (B, Hkv, S, Dh)

        # Share each KV head across group_size query heads by repeating along head dimension
        if self.group_size > 1:
            k = k.repeat_interleave(self.group_size, dim=1)  # (B, Hq, S, Dh)
            v = v.repeat_interleave(self.group_size, dim=1)  # (B, Hq, S, Dh)

        # Attention: (B, Hq, S, Dh) x (B, Hq, Dh, S) -> (B, Hq, S, S)
        attn_scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale

        if attn_mask is not None:
            # attn_mask broadcastable to (B, Hq, S, S). Typical shapes: (S, S) or (B, 1, S, S)
            attn_scores = attn_scores + attn_mask

        attn_weights = F.softmax(attn_scores, dim=-1)
        attn_weights = self.attn_drop(attn_weights)

        # Weighted sum: (B, Hq, S, S) x (B, Hq, S, Dh) -> (B, Hq, S, Dh)
        context = torch.matmul(attn_weights, v)

        # Merge heads: (B, Hq, S, Dh) -> (B, S, Hq*Dh) -> (B, S, d_model)
        context = context.permute(0, 2, 1, 3).contiguous().view(query.size(0), query.size(1), -1)
        out = self.W_o(context)
        return out

if __name__ == "__main__":
    # Example usage:
    # 12 total Q heads, but only 6 heads for K/V => 2 groups of queries
    batch_size, seq_len, d_model = 2, 1024, 768
    num_query_heads = 12
    num_kv_heads = 6 # Must divide num_query_heads

    gqa = GroupedQueryAttention(d_model, num_query_heads, num_kv_heads)
    print("GQA module initialized successfully.")

    query = torch.randn(batch_size, seq_len, d_model)
    key = torch.randn(batch_size, seq_len, d_model)
    value = torch.randn(batch_size, seq_len, d_model)

    out = gqa(query, key, value)
    print("Output shape:", out.shape)
    assert out.shape == (batch_size, seq_len, d_model)
    print("Test passed!")
```
好的！我已把 **Q2：Grouped-Query Attention (GQA)** 按要求实现到你的 `Q2_gqa.py` 里，并跑通了自测示例（输出维度与断言均通过）。

你可以直接下载并运行：
[下载更新后的 Q2_gqa.py](sandbox:/mnt/data/Q2_gqa.py)

### 做了什么

* 新增 `GroupedQueryAttention` 类（最小可用实现）：

  * 支持 `num_query_heads` 与更少的 `num_kv_heads`，自动计算 `group_size = Hq // Hkv`，让每个 K/V 头被 `group_size` 个 Q 头共享（通过 `repeat_interleave` 实现）。
  * 使用 `head_dim = d_model // num_query_heads`，Q 为 `Hq` 个头，K/V 为 `Hkv` 个头，注意力分数按 `1/sqrt(head_dim)` 缩放。
  * 接口：`forward(query, key, value, attn_mask=None)`，输入 `(B, S, d_model)`，输出 `(B, S, d_model)`。
  * 轻量支持 `attn_mask`（可广播到 `(B, Hq, S, S)`）。

### 快速使用

```python
from Q2_gqa import GroupedQueryAttention
gqa = GroupedQueryAttention(d_model=768, num_query_heads=12, num_kv_heads=6)
out = gqa(query, key, value)  # (B, S, 768)
```

### 已本地验证

* 自测脚本中的例子（`B=2, S=1024, d_model=768, Hq=12, Hkv=6`）运行通过，输出为 `(2, 1024, 768)`，与断言一致。
